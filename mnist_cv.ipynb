{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from modelo import modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instancia o modelo\n",
    "model = modelo()\n",
    "# Carregar o modelo treinado\n",
    "model.load_state_dict(torch.load('mnist_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir uma função para pré-processar a imagem\n",
    "def preprocess(image):\n",
    "    # Converte para cinza a imagem\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Image Blurring (Image Smoothing), se usa para remover barulho\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshhold, serve para separar o background do foreground\n",
    "    # cv2.adaptiveThreshold é muito lento, mas dá resultados melhores\n",
    "    _, thresh = cv2.threshold(\n",
    "        blur, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Resize para 28x28 a imagem\n",
    "    resized = cv2.resize(thresh, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Transforma a imagem em um tensor\n",
    "    tensor = torch.from_numpy(resized).float().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Normaliza os pixels para que fiquem entre 0 e 1\n",
    "    tensor /= 255.0\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o objeto de captura de vídeo\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_camera(model, cap):\n",
    "    # Pega as medidas da imagem e define o tamanho do bounding box\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    bbox_size = (180, 180)\n",
    "    bbox = [(int(width // 2 - bbox_size[0] // 2), int(height // 2 - bbox_size[1] // 2)),\n",
    "            (int(width // 2 + bbox_size[0] // 2), int(height // 2 + bbox_size[1] // 2))]\n",
    "\n",
    "    # Começa o vídeo\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # O que está no box é preprocessado e transformado em um tensor\n",
    "        img_cropped = frame[bbox[0][1]:bbox[1][1], bbox[0][0]:bbox[1][0]]\n",
    "        img_tensor = preprocess(img_cropped)\n",
    "\n",
    "        # Predição pelo modelo\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            confidence = torch.nn.functional.softmax(\n",
    "                output, dim=1)[0][predicted] * 100\n",
    "\n",
    "        # Desenha o bounding box apenas para visualização\n",
    "        cv2.rectangle(frame, bbox[0], bbox[1], (0, 255, 0), 3)\n",
    "\n",
    "        # Definido um nível de confiança, o número é mostrado na tela\n",
    "        if confidence > 23:\n",
    "            cv2.putText(frame, str(predicted.item(\n",
    "            )), (bbox[0][0] + 5, bbox[0][1] + 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "\n",
    "        # Cria uma janela para mostrar o vídeo\n",
    "        cv2.imshow('input', frame)\n",
    "\n",
    "        # Cancela o loop quando a tecla 'q' é pressionada\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Sai do programa\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_camera(model, cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
